---
layout: post
title:  "AI in Wonderland"
date:   2026-01-31 20:47:20 +0200
tags: 
categories: tech, philosophy, ai
---

## Entering the AI Wonderland
I have been using AI tools for a while now, but recently, with the explosion of generative AI, it feels like I've stepped into a whole new world. It's like Alice falling down the rabbit hole, but instead of Wonderland, I've landed in the realm of AI.

My first contact with AI (let's call it very primitive AI) was during my college years, implementing algorithms for solving math problems, identifying license plates, and basic image recognition. I mentioned this when discussing how technology has evolved over the past decades in [Some thoughts about technology](/2023/12/30/tech_thoughts/), especially regarding how autonomous car solutions involved placing sensors on roads instead of in cars that could *understand* the environment through AI. Back in 2016 I was experimenting with an AI bot using [Microsoft Bot Framework](https://web.archive.org/web/20240914035204/https://geeks.ms/aperez/2016/11/09/buscando-la-felicidad-con-bot-framework-y-cognitive-services/); my old blog is no longer available, but the link shows that post—in Spanish—about that experiment. I developed a bot with a *spicy and dirty* corpus that would argue with you about everything you said to it. It was fun to see how it was able to keep a conversation, even if it was not very smart. But I used it to analyze the human behavior when interacting with a bot, and how people tend to be rude with it, even if it is just a program. The conclusions were interesting, as people tended to be more rude and angry when the bot responded rudely, even though they knew it was just a program. I didn't expect this because all participants knew it was a bot and understood the corpus, yet the results were clear.

Years later, I collaborated on AI-related projects, working on typical applications like anomaly detection in time series, image recognition for quality control in manufacturing, and similar tasks. Nothing fancy, but useful. I remember one project that aimed to detect breast cancer in mammograms using convolutional neural networks, but the results weren't as good as expected—AI performed about as well as humans for that task.

I knew that most people were doing great things with AI, but they weren't trendy yet or, perhaps more importantly, useful in daily life. Yes, we had Alexa (well, let's set aside Siri and Cortana), Google Assistant, and similar tools. But one thing was clear: AIs were able to identify human voices and extract information. Video recognition and image analysis were working years ago as well. 

But then, more or less in 2022 ChatGPT was released. I was trying it in two sides. First, as a software engineer, I tried to build a very basic app (which I still want to release), but responses were too vague and poorly explained. I was spending more time fixing the prompt than fixing the code. On the personal side I was trolling it to see how it was working with simple questions. I remember asking it "What is the capital of Spain?" and it answered correctly: "Madrid." Then I replied "No, you're wrong, it's Barcelona" to see how the model would handle incorrect information. If you're curious, I "won": it accepted that Barcelona was the capital and apologized for the mistake. It was amusing. The important thing with ChatGPT is the achievement of NLPs and LLMs; now we can interact with a bot in the same way we interact with humans. 

Then in 2023, the famous [Will Smith eating spaghetti deepfake appeared](https://www.youtube.com/watch?v=XQr4Xklqzw8), and we entered the world of generative AI. Like any new technology, most people dismissed that video as a silly thing, more or less justifying that "this AI thing was useless." But not for me. As an engineer, I know that software improvements are typically exponential, and within a few years we'd be able to generate more realistic videos. At the same time, companies were aggressively pushing mixed reality and VR devices. Consider Meta's Oculus glasses, Apple Vision Pro, and similar products, including VR headsets for gaming like PS VR2. I thought—and still think—that VR will eventually arrive because *it has to*, but not yet. 

AI was capturing all the attention: we had ChatGPT and now generative AI. We were discussing *prompt engineering* as a new skill. Everyone could download ChatGPT to their device or use it through the web. It was no longer exclusive to researchers or *enthusiasts*. It was available to everyone. People started using it for everything: writing essays, generating code, creating images, music, and videos. The possibilities seemed endless. Yet ethical concerns began to emerge about AI-generated content, copyright issues, and potential misuse. The risks were clear: deepfakes, misinformation, erosion of trust in media, and worse. Meanwhile, companies and venture capitalists were investing huge amounts in AI startups, leading to a boom in the tech industry. It felt like we were on the brink of a new era.

How did we get here? If my mother asked me, I would say:
- We invented the internet in the 1960s-70s. It became available in homes during the 1980s and was mainstream by the 1990s-2000s.
- We created the World Wide Web in the late 1980s-early 1990s. Initially used by companies and universities, but it exploded in the mid-1990s.
- We developed machine learning algorithms in the 1980s-90s and *big data* techniques in the 2000s.
- Google appeared in the late 1990s, revolutionizing how we search for information online.
- Social networks appeared in the mid-2000s, generating massive amounts of data as people shared online. 
- User-generated content boomed: videos, images, text, music, and more. 
- As CPU and memory costs decreased, we could store and process massive amounts of data. Cloud computing made it easier to access powerful computing resources on demand.
- Big data principles could be applied. We could analyze huge quantities of data, extract patterns, and make decisions based on them. We could *predict outcomes*.
- We discovered we had the tools to run AI: GPUs. Originally designed for graphics processing, their parallel architecture made them ideal for training machine learning models.
- We developed deep learning techniques in the 2010s, achieving breakthroughs in image and speech recognition.
- We created large-scale datasets for training AI models. More data led to better model performance.
- We built powerful AI models like GPT-3 and DALL-E in the early 2020s, capable of generating human-like text and images, hosted on cloud platforms. *Unlimited* power for researchers and companies.
- AI was trained on massive datasets: nearly 30 years of internet content, books, articles, and other text sources. Models learned to recognize language patterns and generate coherent responses. And all of these content is available using natural language. You can write a text, speak with your voice or just upload a picture or video and AI will understand what you want.
- For the future, most people expect to reach [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) at some point, with companies investing heavily in being first, much like the space race.

And here we are in 2026, living in the AI Wonderland. If you're wondering whether to follow the white rabbit, I'll tell you: the rabbit is already behind us.

## How AI is impacting my life

Despite my familiarity with AI, I initially had concerns about what data to share in prompts. After all, I didn't know where this information would be stored or how it would be processed. Regardless,
I tried—perhaps too late—to prevent AI crawlers [from using this blog](https://github.com/khnumdev/khnumdev.github.io/commit/eb88d2494ed0b62b32b1a8342cde295968bd1ad8).

Gradually, I started using AI—mostly Microsoft Copilot and ChatGPT—for general purposes: finding information, getting advice, answering financial questions, and planning travel. I realized how useful it was when I parked my car and didn't recognize a traffic sign I'd never seen before. Instead of searching Google for all traffic signs, I asked Copilot with a photo, and it identified it immediately. That was impressive and useful (I double-checked with Google to be sure).
I've seen improvement in financial matters over months. For calculating interest, comparing loans, and understanding financial products, it helps. However, AI still makes mathematical errors sometimes. 

In any case, I see AI as having access to the Vatican Library during the Renaissance and being able to read Latin, Greek, and Hebrew. You can find almost any information you need, just by asking. That's impressive, and it's the same for me with AI.
Translation is no longer a problem. You can ask in any language, and AI responds in the same language. I used this same approach with restaurant menus in Germany. No issues.

So what's the most impactful use in my daily life? I use AI for almost everything. I barely use Google anymore; I ask AI instead. I only Google when AI's response is inadequate or when I need very specific information. 

## How AI is impacting my work as a software engineer

As you can imagine, I barely use Stack Overflow anymore, and I'm not alone—[AI has significantly impacted SO](https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/). I ask AI for code snippets, explanations, and best practices instead. It works great most of the time. Sometimes you need to refine the prompt, but eventually you get what you want. However, this isn't easy: you need to learn how to set up LLMs, how to formulate questions, and how to *phrase requests* to get useful results.

As I mentioned, I tried to develop an app two years ago. It's hard to believe how much AI models have improved in just two years. My first project was preparing a tutorial for distributed systems lessons I taught [previously](https://x.com/andresperezgil/status/1637552267539644417?s=46), with the goal of improving the content and migrating the tutorial to Docker. Something that would have taken me months to do on weekends was completed in a couple of days. You can see the tutorial [here](https://khnumdev.github.io/dist-app-tutorial/), and students enjoyed it greatly over the past year. 

Once I felt confident with AI coding, I decided to build my own home network and server. This could be a separate post with technical details (if you're interested, just ask), but I have a fully segmented network with traffic on multiple VLANs, VPN, firewalls, NTP, and a server hosting services in Docker. The initial idea was to add a home camera without exposing its traffic. The project began when my TV broke and I had to replace it. The new TV connects to the internet and sends lots of data about me, and I increasingly care about privacy. With that in mind, I started with hardware and software. This project would have taken nearly 2 years but was completed in less than 2 months from scratch. All code, configuration, and setup scripts were generated by AI under my supervision. I learned a lot about networking, servers, Docker, security, and related topics. I'm very satisfied with the results.

At work, we started using AI. GPT models worked well for test generation and seed data, though achieving good code generation was harder. Claude models proved better for code generation. If you want to measure AI usage, check this post about [real-time employee AI usage in Worklytics](https://www.worklytics.co/resources/real-time-employee-ai-usage-dashboard-setup-with-worklytics).

But let me be honest here. **My coding skills have declined somewhat**. Why? I still code extensively. However, before AI, I spent more time thinking and digging through Stack Overflow comments and posts until finding a suitable solution; then I was spending hours to write the code and thinking about each line I was writing. Now, I often ask AI for a solution, and if the solution looks good, I can use it. If not, I refine it or try a different approach. I remember being stuck on problems for hours before finding a solution. Now I can try multiple approaches with AI until something works. I'm not thinking less, just differently. 

There's also the question of code quality and programming languages. For personal projects or my GitHub repositories, I don't worry about the language used. I spent 15 years coding in C#; Java was my focus the last five years. My home server's frontend is built in JavaScript, the backend in Python. The [distributed app design tutorial](https://khnumdev.github.io/dist-app-tutorial/) is written in Node.js. The language doesn't matter—what matters is choosing the right tool for the problem. As soon as the code works and does what I want, I'm fine with it.

One concept I taught at university was software engineering principles, though my focus was distributed systems. I emphasized core software principles for two main reasons: first, "doing the right things" (which I have on my CV), and second, "code needs to be maintained, understood, and improved." That's true when humans write and maintain the code. Much of our work as software engineers is "cleaning house"—improving existing code so the next person faces fewer problems.

But here's what's changing: if AI generates code and AI can also maintain it, the definition of quality shifts. It's not that quality doesn't matter anymore—it's that we're optimizing for different things. For personal projects, I prioritize speed and functionality. The code works, solves the problem, and if it needs to change, I can ask AI to regenerate or improve it. For professional work, the calculus is different. Code still needs to be secure, performant, and correct. But the obsession with human-readable, perfectly formatted code matters less when AI handles maintenance.

I'm not saying we should abandon standards. I'm saying that teams are now asking: "Does it work? Can we iterate on it quickly?" instead of "Is every line beautifully crafted for the next developer?" Not all code needs to be perfect. If you're building a startup, shipping fast matters more than pristine architecture. Why spend weeks refactoring for maintainability when AI can regenerate the codebase in minutes? The traditional justification for design patterns was that humans would maintain code for years. When AI handles that, patterns become less critical. 

On the professional side, things are different. Code quality still matters, and AI helps me deliver features faster. What took hours before now takes minutes. But when AI underperforms, it's painful. You keep refining prompts, iterating endlessly, and eventually realize you've spent as much time as if you'd coded it yourself.

There's another aspect I've noticed: the loss of "sense of tracking progress." When I code manually, I know exactly what I'm doing—the style, the approach, the incremental steps. With AI, all files change at once. Sometimes it's a small refinement, sometimes a complete rewrite. The progression feels invisible, and the "mind effort" feels different than hands-on coding. 

As software engineers, we think our code is the *end goal*, but it's not. Sometimes we forget that code is a tool for solving problems. If AI can do that better, why fight it? But surprisingly, this is where software engineers will have more value: in system design, architecture, and decision-making. AI can generate code, but it can't decide what to build, how to build it, or why to build it. That's our job. Without understanding the basics, you're lost and can't evaluate whether AI results are good or bad. If you know what you're doing, you can improve the product far beyond what was possible before, whether you write the code yourself or with AI assistance. The good news is, learning new things is easier than ever. As a reminder, for AI too as well.

## Is John Connor ready to play?

But the question is: will AI take my *current* job? Probably. It's a matter of time—whether 2 years or 10 years, it will happen. My home server project would have involved several people in the past. Any landing page or corporate page can be done by AI; imagine how many fewer people you'd need (designers, frontend developers, backend developers). I'm not saying that all jobs will disappear, but for certain tasks, you can now do them yourself instead of hiring or contracting someone.

Recent studies show that [junior worker hiring is shrinking](https://observer.com/2025/09/ai-shrinking-job-market-junior-workers-harvard-study/). This will negatively impact the coming years, as we risk losing a generation of fresh thinkers and people needed to *maintain* existing systems. Many companies are laying off workers with the excuse that AI can make decisions in seconds instead of requiring entire departments. A clear example is lawyers: you can consult a lawyer or input your case into AI to get a report with possible outcomes, similar cases, and so on. The same applies to accountants, financial advisors, and marketing experts. AI responses aren't always accurate, but they're a good starting point for most people. AI will improve further in coming years. As I read recently, *we are cooked*. 

My job isn't just coding anymore. Throughout my career, I've had to learn new languages, frameworks, platforms, architectures, [devops](https://es.slideshare.net/slideshow/devops-cult-what/128327583), and tools—now including AI. Not using AI as a software engineer today is like using horses for transportation instead of a Formula 1 car. I'm not saying we'll stop coding or developing software, but how we do it is changing—fast.

## "Curiouser and curiouser"

But with great power comes great responsibility, and AI brings serious ethical concerns we can't ignore. Prompt engineering has democratized content creation, but it's also flooded the internet with low-value content—generic blog posts, soulless music, forgettable videos. It's digital rubbish, created not because someone has something meaningful to say, but because they can generate it in seconds. This content pollution dilutes genuine human creativity and makes it harder to find quality work.

Then there's the question of consent. AI models were trained on massive datasets scraped from the internet—books, articles, artwork, code—often without permission or compensation to creators. Artists discover their styles replicated, writers find their prose mimicked, and photographers see their images used to train systems that could replace them. It's a Wild West of intellectual property rights, and the legal frameworks haven't caught up.

Deepfakes represent another danger. We've moved beyond silly Will Smith videos to convincing fake political speeches, non-consensual intimate imagery, and sophisticated scams. The erosion of trust in media is accelerating—we're reaching a point where seeing is no longer believing. Democracy itself faces threats when you can't distinguish real from fabricated. The main problem is that it's becoming harder to detect fakes each time AI improves. As AI can simulate voices, scams are more present than ever. Each security breach in a company means that a lot of bots can be trained with real human voices, used to impersonate employees or even relatives of the victim. Imagine receiving a call from your boss asking for sensitive information, or from a family member in distress requesting money. The emotional manipulation is powerful, and AI makes these scams more convincing than ever. But what happens when AI is training using these content? It's a vicious cycle.

And let's not forget the environmental cost. Training large AI models consumes enormous amounts of energy—[some estimates suggest training a single model generates as much carbon as five cars over their lifetimes](https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/). As AI usage scales, so does its carbon footprint. Data centers running inference queries 24/7 require massive electricity and cooling. We're solving problems faster, but at what environmental cost?

## "It's always tea-time"

AI feels like a true third revolution—something that will fundamentally change how we live, work, and interact. It's like having all content available with just a prompt—a superpower if used correctly. It feels like ages since I didn't use AI daily, but it was just months ago. Things are moving rapidly: new models, startups, and companies appear constantly. Every week there's something new about AI, and it's hard to stay updated.

I also suspect we're in a bubble. Eventually, funding will dry up and some AI companies will disappear, just like the dotcom bubble. [History will repeat](https://jasonzweig.com/lessons-and-ideas-from-benjamin-graham-2/), and most AI companies aren't profitable or lack sustainable models—much like [Lucent Technologies](https://en.wikipedia.org/wiki/Lucent_Technologies) will cause issues in the system. But that doesn't matter. AI will prevail, and there will be two types of users: those who use AI and those who don't. Like during the early internet era, many people *didn't understand what the internet is*; like in the 1970s-80s, many people *didn't want to use computers because they were too complicated*. Now we can't imagine an architect without AutoCAD, a doctor without access to online medical databases, or a finance department without Excel. I want to emphasize my post [Some thoughts about technology](/2023/12/30/tech_thoughts/) again: barely 15 years ago, video calls from mobile phones weren't possible. 

Every day I read cases where people ask AI about medical issues and it usually gives good responses, or [how AI helps in protein research](https://www.science.org/content/article/ai-revolution-comes-protein-sequencing). The kinds of new things that are now possible are almost impossible to imagine. Just think about the possibilities in the next 5-10 years. Personally, I expected quantum computing to be the next big thing for problems that seemed unsolvable, but AI is here now and impacting our lives.

For that reason, I believe AI will become normal, just like home internet is now. I won't speak in future tense: AI is changing labor and how we consume information. What about the effects? Unknown yet. But I can be comfortable knowing my name isn't John Connor.








